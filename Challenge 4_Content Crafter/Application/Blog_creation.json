{"id":"9bfd43b2-0b96-47b2-8f88-34cbe2fce597","data":{"nodes":[{"id":"SequentialTaskAgentComponent-IlRHa","type":"genericNode","position":{"x":952.0665756721137,"y":439.99584558094057},"data":{"type":"SequentialTaskAgentComponent","node":{"template":{"_type":"Component","llm":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"llm","value":"","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"Language model that will run the agent.","title_case":false,"type":"other","_input_type":"HandleInput"},"previous_task":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"previous_task","value":"","display_name":"Previous Task","advanced":false,"input_types":["SequentialTask"],"dynamic":false,"info":"The previous task in the sequence (for chaining).","title_case":false,"type":"other","_input_type":"HandleInput"},"tools":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"tools","value":[],"display_name":"Tools","advanced":false,"input_types":["Tool"],"dynamic":false,"info":"Tools at agent's disposal","title_case":false,"type":"other","_input_type":"HandleInput"},"agent_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"agent_kwargs","value":{},"display_name":"Agent kwargs","advanced":true,"dynamic":false,"info":"Additional kwargs for the agent.","title_case":false,"type":"dict","_input_type":"DictInput"},"allow_code_execution":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_code_execution","value":false,"display_name":"Allow Code Execution","advanced":true,"dynamic":false,"info":"Whether the agent is allowed to execute code.","title_case":false,"type":"bool","_input_type":"BoolInput"},"allow_delegation":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_delegation","value":false,"display_name":"Allow Delegation","advanced":true,"dynamic":false,"info":"Whether the agent is allowed to delegate tasks to other agents.","title_case":false,"type":"bool","_input_type":"BoolInput"},"async_execution":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"async_execution","value":false,"display_name":"Async Execution","advanced":true,"dynamic":false,"info":"Boolean flag indicating asynchronous task execution.","title_case":false,"type":"bool","_input_type":"BoolInput"},"backstory":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"backstory","value":"You are a professional editor who is passionate about fine-tuning language. Ensure that every piece of content is grammatically correct, well-structured, and engaging to readers.","display_name":"Backstory","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The backstory of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from crewai import Agent, Task\n\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"expected_output":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"expected_output","value":"1. An error-free piece of content that is grammatically correct.\n2. A more reader-friendly article, with improvements to sentence structure, word choice, and overall clarity.\n3. A report indicating whether the content is original or contains copied sections.\n\n","display_name":"Expected Task Output","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Clear definition of expected task outcome.","title_case":false,"type":"str","_input_type":"MultilineInput"},"goal":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"goal","value":"Proofread, edit, and enhance content to ensure it is free of errors and optimized for readability and engagement.  ","display_name":"Goal","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The objective of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":true,"display_name":"Memory","advanced":true,"dynamic":false,"info":"Whether the agent should have memory or not","title_case":false,"type":"bool","_input_type":"BoolInput"},"role":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"role","value":"Content Editor  ","display_name":"Role","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The role of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"task_description":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"task_description","value":"1. Review the content for grammatical errors, spelling mistakes, and punctuation issues.\n2. Improve readability by simplifying sentences, breaking up long paragraphs, and suggesting clearer phrasing.\n3. Scan the content for potential plagiarism using a plagiarism detection tool.","display_name":"Task Description","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Descriptive text detailing task's purpose and execution.","title_case":false,"type":"str","_input_type":"MultilineInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Creates a CrewAI Task and its associated Agent.","icon":"CrewAI","base_classes":["SequentialTask"],"display_name":"Sequential Task Agent","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["SequentialTask"],"selected":"SequentialTask","name":"task_output","display_name":"Sequential Task","method":"build_agent_and_task","value":"__UNDEFINED__","cache":true}],"field_order":["role","goal","backstory","tools","llm","memory","verbose","allow_delegation","allow_code_execution","agent_kwargs","task_description","expected_output","async_execution","previous_task"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"SequentialTaskAgentComponent-IlRHa"},"selected":false,"width":384,"height":786,"positionAbsolute":{"x":952.0665756721137,"y":439.99584558094057},"dragging":false},{"id":"ChatOutput-5AUHf","type":"genericNode","position":{"x":2082.080282906332,"y":699.1466848999942},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\r\nfrom langflow.inputs import BoolInput\r\nfrom langflow.io import DropdownInput, MessageTextInput, Output\r\nfrom langflow.memory import store_message\r\nfrom langflow.schema.message import Message\r\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\r\n\r\nclass ChatOutput(ChatComponent):\r\n    display_name = \"Chat Output\"\r\n    description = \"Display a chat message in the Playground.\"\r\n    icon = \"ChatOutput\"\r\n    name = \"ChatOutput\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"input_value\",\r\n            display_name=\"Text\",\r\n            info=\"Message to be passed as output. Can include image URLs.\",\r\n        ),\r\n        BoolInput(\r\n            name=\"should_store_message\",\r\n            display_name=\"Store Messages\",\r\n            info=\"Store the message in the history.\",\r\n            value=True,\r\n            advanced=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"sender\",\r\n            display_name=\"Sender Type\",\r\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\r\n            value=MESSAGE_SENDER_AI,\r\n            advanced=True,\r\n            info=\"Type of sender.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"sender_name\",\r\n            display_name=\"Sender Name\",\r\n            info=\"Name of the sender.\",\r\n            value=MESSAGE_SENDER_AI,\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"session_id\",\r\n            display_name=\"Session ID\",\r\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"data_template\",\r\n            display_name=\"Data Template\",\r\n            value=\"{text}\",\r\n            advanced=True,\r\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\r\n        ),\r\n    ]\r\n    outputs = [\r\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\r\n    ]\r\n\r\n    def message_response(self) -> Message:\r\n        # Extract the input value (text) and check for image URLs\r\n        message_text = self.input_value\r\n        images = self.extract_image_urls(message_text)  # Custom method to extract images\r\n        \r\n        # Format the message text to include images if any\r\n        formatted_text = self.format_message_with_images(message_text, images)\r\n        \r\n        message = Message(\r\n            text=formatted_text,\r\n            sender=self.sender,\r\n            sender_name=self.sender_name,\r\n            session_id=self.session_id,\r\n        )\r\n        \r\n        if (\r\n            self.session_id\r\n            and isinstance(message, Message)\r\n            and isinstance(message.text, str)\r\n            and self.should_store_message\r\n        ):\r\n            store_message(\r\n                message,\r\n                flow_id=self.graph.flow_id,\r\n            )\r\n            self.message.value = message\r\n\r\n        self.status = message\r\n        return message\r\n\r\n    def extract_image_urls(self, text: str):\r\n        # Implement logic to extract image URLs from the text\r\n        # This could be a regex search or a simple check for URLs\r\n        # Example: return a list of found URLs\r\n        return [url for url in text.split() if url.startswith(\"http\") and (url.endswith(\".jpg\") or url.endswith(\".png\") or url.endswith(\".gif\"))]\r\n\r\n    def format_message_with_images(self, text: str, images: list):\r\n        # Use HTML for image rendering\r\n        formatted_message = text\r\n        for img_url in images:\r\n            formatted_message += f'<br><img src=\"{img_url}\" alt=\"Image\" style=\"max-width: 100%; height: auto;\">'\r\n        return formatted_message","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output. Can include image URLs.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.0.18"},"id":"ChatOutput-5AUHf"},"selected":false,"width":384,"height":302,"positionAbsolute":{"x":2082.080282906332,"y":699.1466848999942},"dragging":false},{"id":"Prompt-rFQzV","type":"genericNode","position":{"x":-148.60371420631765,"y":403.5086557324992},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are a well-trained AI writer. Given a {topic} from the user, generates a full-length blog post, article, or any type of written content with the appropriate structure.\nOrganize the content into subheadings, bullet points, and paragraphs to make it scannable and easy to read. Use the tool you have to search for the information you require to write a complete blog. ","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"topic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"topic","display_name":"topic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["topic"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-rFQzV"},"selected":false,"width":384,"height":416,"dragging":false,"positionAbsolute":{"x":-148.60371420631765,"y":403.5086557324992}},{"id":"GroqModel-NucS9","type":"genericNode","position":{"x":2.8084859920339795,"y":-405.2267865793168},"data":{"node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name=\"groq_api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\"),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return []\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"groq_api_key\", \"groq_api_base\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key).get_secret_value(),\n            streaming=stream,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"trace_as_metadata":true,"options":["gemma-7b-it","llama3-groq-8b-8192-tool-use-preview","llama-3.3-70b-specdec","llama3-groq-70b-8192-tool-use-preview","llama3-8b-8192","llama-guard-3-8b","whisper-large-v3-turbo","llama-3.3-70b-versatile","llama-3.1-8b-instant","whisper-large-v3","mixtral-8x7b-32768","gemma2-9b-it","llama-3.2-1b-preview","llama-3.1-70b-versatile","llama3-70b-8192","llama-3.2-11b-vision-preview","llama-3.2-3b-preview","distil-whisper-large-v3-en","llama-3.2-90b-vision-preview"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama3-70b-8192","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput","load_from_db":false},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":true,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","load_from_db":false},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","output_parser"],"beta":false,"edited":true,"lf_version":"1.0.18"},"type":"GroqModel","id":"GroqModel-NucS9"},"selected":false,"width":384,"height":621,"dragging":false,"positionAbsolute":{"x":2.8084859920339795,"y":-405.2267865793168}},{"id":"SequentialTaskAgentComponent-8hmxv","type":"genericNode","position":{"x":427.258433128025,"y":422.9515712572735},"data":{"node":{"template":{"_type":"Component","llm":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"llm","value":"","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"Language model that will run the agent.","title_case":false,"type":"other","_input_type":"HandleInput"},"previous_task":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"previous_task","value":"","display_name":"Previous Task","advanced":false,"input_types":["SequentialTask"],"dynamic":false,"info":"The previous task in the sequence (for chaining).","title_case":false,"type":"other","_input_type":"HandleInput"},"tools":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"tools","value":[],"display_name":"Tools","advanced":false,"input_types":["Tool"],"dynamic":false,"info":"Tools at agent's disposal","title_case":false,"type":"other","_input_type":"HandleInput"},"agent_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"agent_kwargs","value":{},"display_name":"Agent kwargs","advanced":true,"dynamic":false,"info":"Additional kwargs for the agent.","title_case":false,"type":"dict","_input_type":"DictInput"},"allow_code_execution":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_code_execution","value":false,"display_name":"Allow Code Execution","advanced":true,"dynamic":false,"info":"Whether the agent is allowed to execute code.","title_case":false,"type":"bool","_input_type":"BoolInput"},"allow_delegation":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_delegation","value":false,"display_name":"Allow Delegation","advanced":true,"dynamic":false,"info":"Whether the agent is allowed to delegate tasks to other agents.","title_case":false,"type":"bool","_input_type":"BoolInput"},"async_execution":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"async_execution","value":false,"display_name":"Async Execution","advanced":true,"dynamic":false,"info":"Boolean flag indicating asynchronous task execution.","title_case":false,"type":"bool","_input_type":"BoolInput"},"backstory":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"backstory","value":"You are a well-trained AI writer. You specialize in understanding user prompts and converting them into written content with the appropriate tone and structure. Use the tool you have to search for the information you require to write a complete blog. ","display_name":"Backstory","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The backstory of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from crewai import Agent, Task\n\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"expected_output":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"expected_output","value":"1. A structured, coherent blog post or article of 600-1,500 words that is relevant to the topic, contains the necessary depth in markdown format.\n2. Text that matches the user's tone and style preferences, with appropriate language, vocabulary, and structure. \n3. A clear and well-structured article with headings, subheadings, lists, and paragraphs.\n4. A blog with the below contents in markdown format.\na. Title: A clear and engaging title for the blog post.\nb. Introduction: A brief introduction that outlines the main topic and purpose of the post.\nc. Body Content: Well-structured sections with headings and subheadings that cover the main points of the topic. Each section should have informative content.\nd. Conclusion: A summary that wraps up the discussion and provides any final thoughts or calls to action.\nf. References: Any sources or references used in the post, if applicable.","display_name":"Expected Task Output","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Clear definition of expected task outcome.","title_case":false,"type":"str","_input_type":"MultilineInput"},"goal":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"goal","value":"Generate coherent, relevant, and high-quality written content based on user input. ","display_name":"Goal","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The objective of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":true,"display_name":"Memory","advanced":true,"dynamic":false,"info":"Whether the agent should have memory or not","title_case":false,"type":"bool","_input_type":"BoolInput"},"role":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"role","value":"Content Creator","display_name":"Role","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The role of the agent.","title_case":false,"type":"str","_input_type":"MultilineInput"},"task_description":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"task_description","value":"","display_name":"Task Description","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Descriptive text detailing task's purpose and execution.","title_case":false,"type":"str","_input_type":"MultilineInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Creates a CrewAI Task and its associated Agent.","icon":"CrewAI","base_classes":["SequentialTask"],"display_name":"Sequential Task Agent","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["SequentialTask"],"selected":"SequentialTask","name":"task_output","display_name":"Sequential Task","method":"build_agent_and_task","value":"__UNDEFINED__","cache":true}],"field_order":["role","goal","backstory","tools","llm","memory","verbose","allow_delegation","allow_code_execution","agent_kwargs","task_description","expected_output","async_execution","previous_task"],"beta":false,"edited":false,"lf_version":"1.0.18"},"type":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-8hmxv","description":"Creates a CrewAI Task and its associated Agent.","display_name":"Sequential Task Agent"},"selected":false,"width":384,"height":786,"positionAbsolute":{"x":427.258433128025,"y":422.9515712572735},"dragging":false},{"id":"SequentialCrewComponent-mavMj","type":"genericNode","position":{"x":1520.318290956398,"y":694.794385697444},"data":{"node":{"template":{"_type":"Component","function_calling_llm":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"function_calling_llm","value":"","display_name":"Function Calling LLM","advanced":true,"input_types":["LanguageModel"],"dynamic":false,"info":"Turns the ReAct CrewAI agent into a function-calling agent","title_case":false,"type":"other","_input_type":"HandleInput"},"tasks":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"tasks","value":"","display_name":"Tasks","advanced":false,"input_types":["SequentialTask"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from crewai import Agent, Crew, Process, Task  # type: ignore\n\nfrom langflow.base.agents.crewai.crew import BaseCrewComponent\nfrom langflow.io import HandleInput\nfrom langflow.schema.message import Message\n\n\nclass SequentialCrewComponent(BaseCrewComponent):\n    display_name: str = \"Sequential Crew\"\n    description: str = \"Represents a group of agents with tasks that are executed sequentially.\"\n    documentation: str = \"https://docs.crewai.com/how-to/Sequential/\"\n    icon = \"CrewAI\"\n\n    inputs = BaseCrewComponent._base_inputs + [\n        HandleInput(name=\"tasks\", display_name=\"Tasks\", input_types=[\"SequentialTask\"], is_list=True),\n    ]\n\n    def get_tasks_and_agents(self) -> tuple[list[Task], list[Agent]]:\n        return self.tasks, [task.agent for task in self.tasks]\n\n    def build_crew(self) -> Message:\n        tasks, agents = self.get_tasks_and_agents()\n        crew = Crew(\n            agents=agents,\n            tasks=tasks,\n            process=Process.sequential,\n            verbose=self.verbose,\n            memory=self.memory,\n            cache=self.use_cache,\n            max_rpm=self.max_rpm,\n            share_crew=self.share_crew,\n            function_calling_llm=self.function_calling_llm,\n            step_callback=self.get_step_callback(),\n            task_callback=self.get_task_callback(),\n        )\n        return crew\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"max_rpm":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_rpm","value":100,"display_name":"Max RPM","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":false,"display_name":"Memory","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"share_crew":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"share_crew","value":false,"display_name":"Share Crew","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"use_cache":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"use_cache","value":true,"display_name":"Cache","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":0,"display_name":"Verbose","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"}},"description":"Represents a group of agents with tasks that are executed sequentially.","icon":"CrewAI","base_classes":["Message"],"display_name":"Sequential Crew","documentation":"https://docs.crewai.com/how-to/Sequential/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["verbose","memory","use_cache","max_rpm","share_crew","function_calling_llm","tasks"],"beta":false,"edited":false,"lf_version":"1.0.18"},"type":"SequentialCrewComponent","id":"SequentialCrewComponent-mavMj","description":"Represents a group of agents with tasks that are executed sequentially.","display_name":"Sequential Crew"},"selected":false,"width":384,"height":288,"positionAbsolute":{"x":1520.318290956398,"y":694.794385697444},"dragging":false},{"id":"TextInput-KCZ3t","type":"genericNode","position":{"x":-767.5661303752256,"y":420.29485146267973},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Topic\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"AI for Gespatial applications","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Topic","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.0.18"},"type":"TextInput","id":"TextInput-KCZ3t"},"selected":false,"width":384,"height":302,"positionAbsolute":{"x":-767.5661303752256,"y":420.29485146267973},"dragging":false},{"id":"GoogleSerperAPI-TM8V4","type":"genericNode","position":{"x":-497.83874044866945,"y":-264.15531837740184},"data":{"type":"GoogleSerperAPI","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Union\n\nfrom langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\n\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import SecretStrInput, MultilineInput, IntInput\nfrom langflow.schema import Data\nfrom langflow.field_typing import Tool\n\n\nclass GoogleSerperAPIComponent(LCToolComponent):\n    display_name = \"Google Serper API\"\n    description = \"Call the Serper.dev Google Search API.\"\n    name = \"GoogleSerperAPI\"\n\n    inputs = [\n        SecretStrInput(name=\"serper_api_key\", display_name=\"Serper API Key\", required=True),\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        IntInput(name=\"k\", display_name=\"Number of results\", value=4, required=True),\n    ]\n\n    def run_model(self) -> Union[Data, list[Data]]:\n        wrapper = self._build_wrapper()\n        results = wrapper.results(query=self.input_value)\n        list_results = results.get(\"organic\", [])\n        data = [Data(data=result, text=result[\"snippet\"]) for result in list_results]\n        self.status = data\n        return data\n\n    def build_tool(self) -> Tool:\n        wrapper = self._build_wrapper()\n        return Tool(\n            name=\"google_search\",\n            description=\"Search Google for recent results.\",\n            func=wrapper.run,\n        )\n\n    def _build_wrapper(self):\n        return GoogleSerperAPIWrapper(serper_api_key=self.serper_api_key, k=self.k)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"k":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"name":"k","value":4,"display_name":"Number of results","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"serper_api_key":{"load_from_db":false,"required":true,"placeholder":"","show":true,"name":"serper_api_key","value":"","display_name":"Serper API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"}},"description":"Call the Serper.dev Google Search API.","base_classes":["Data","Tool"],"display_name":"Google Serper API","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"api_run_model","display_name":"Data","method":"run_model","value":"__UNDEFINED__","cache":true},{"types":["Tool"],"selected":"Tool","name":"api_build_tool","display_name":"Tool","method":"build_tool","value":"__UNDEFINED__","cache":true}],"field_order":["serper_api_key","input_value","k"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"GoogleSerperAPI-TM8V4"},"selected":false,"width":384,"height":519,"dragging":false,"positionAbsolute":{"x":-497.83874044866945,"y":-264.15531837740184}}],"edges":[{"source":"GroqModel-NucS9","sourceHandle":"{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-NucS9œ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"SequentialTaskAgentComponent-IlRHa","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"SequentialTaskAgentComponent-IlRHa","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-NucS9","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-NucS9{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-NucS9œ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-IlRHa{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"Prompt-rFQzV","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-rFQzVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"SequentialTaskAgentComponent-8hmxv","targetHandle":"{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"task_description","id":"SequentialTaskAgentComponent-8hmxv","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-rFQzV","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-rFQzV{œdataTypeœ:œPromptœ,œidœ:œPrompt-rFQzVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-8hmxv{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"GroqModel-NucS9","sourceHandle":"{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-NucS9œ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"SequentialTaskAgentComponent-8hmxv","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"SequentialTaskAgentComponent-8hmxv","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-NucS9","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-NucS9{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-NucS9œ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-8hmxv{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"SequentialTaskAgentComponent-8hmxv","sourceHandle":"{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}","target":"SequentialTaskAgentComponent-IlRHa","targetHandle":"{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"previous_task","id":"SequentialTaskAgentComponent-IlRHa","inputTypes":["SequentialTask"],"type":"other"},"sourceHandle":{"dataType":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-8hmxv","name":"task_output","output_types":["SequentialTask"]}},"id":"reactflow__edge-SequentialTaskAgentComponent-8hmxv{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-IlRHa{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"SequentialCrewComponent-mavMj","sourceHandle":"{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-mavMjœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-5AUHf","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-5AUHfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-5AUHf","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"SequentialCrewComponent","id":"SequentialCrewComponent-mavMj","name":"output","output_types":["Message"]}},"id":"reactflow__edge-SequentialCrewComponent-mavMj{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-mavMjœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-5AUHf{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-5AUHfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"TextInput-KCZ3t","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KCZ3tœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-rFQzV","targetHandle":"{œfieldNameœ:œtopicœ,œidœ:œPrompt-rFQzVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"topic","id":"Prompt-rFQzV","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-KCZ3t","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-KCZ3t{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KCZ3tœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-rFQzV{œfieldNameœ:œtopicœ,œidœ:œPrompt-rFQzVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"GoogleSerperAPI-TM8V4","sourceHandle":"{œdataTypeœ:œGoogleSerperAPIœ,œidœ:œGoogleSerperAPI-TM8V4œ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}","target":"SequentialTaskAgentComponent-8hmxv","targetHandle":"{œfieldNameœ:œtoolsœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"tools","id":"SequentialTaskAgentComponent-8hmxv","inputTypes":["Tool"],"type":"other"},"sourceHandle":{"dataType":"GoogleSerperAPI","id":"GoogleSerperAPI-TM8V4","name":"api_build_tool","output_types":["Tool"]}},"id":"reactflow__edge-GoogleSerperAPI-TM8V4{œdataTypeœ:œGoogleSerperAPIœ,œidœ:œGoogleSerperAPI-TM8V4œ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}-SequentialTaskAgentComponent-8hmxv{œfieldNameœ:œtoolsœ,œidœ:œSequentialTaskAgentComponent-8hmxvœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}","className":""},{"source":"SequentialTaskAgentComponent-IlRHa","sourceHandle":"{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}","target":"SequentialCrewComponent-mavMj","targetHandle":"{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-mavMjœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"tasks","id":"SequentialCrewComponent-mavMj","inputTypes":["SequentialTask"],"type":"other"},"sourceHandle":{"dataType":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-IlRHa","name":"task_output","output_types":["SequentialTask"]}},"id":"reactflow__edge-SequentialTaskAgentComponent-IlRHa{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-IlRHaœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialCrewComponent-mavMj{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-mavMjœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}"}],"viewport":{"x":-208.8281314210924,"y":-538.8582571938706,"zoom":0.5756534541700179}},"description":"Sculpting Language with Precision.","name":"Blog_creation","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}